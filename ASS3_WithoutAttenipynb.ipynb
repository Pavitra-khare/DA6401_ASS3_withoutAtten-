{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8190591,
          "sourceType": "datasetVersion",
          "datasetId": 4850432
        },
        {
          "sourceId": 8303996,
          "sourceType": "datasetVersion",
          "datasetId": 4933043
        },
        {
          "sourceId": 8304151,
          "sourceType": "datasetVersion",
          "datasetId": 4933099
        },
        {
          "sourceId": 8304593,
          "sourceType": "datasetVersion",
          "datasetId": 4933272
        },
        {
          "sourceId": 8407265,
          "sourceType": "datasetVersion",
          "datasetId": 5003045
        },
        {
          "sourceId": 11806983,
          "sourceType": "datasetVersion",
          "datasetId": 7415098
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavitra-khare/DA6401_ASS3_withoutAtten-/blob/main/ASS3_WithoutAttenipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and login"
      ],
      "metadata": {
        "id": "VejTsKhNLm2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install wandb\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.259702Z",
          "iopub.execute_input": "2025-05-17T22:49:14.260309Z",
          "iopub.status.idle": "2025-05-17T22:49:14.263472Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.260285Z",
          "shell.execute_reply": "2025-05-17T22:49:14.262655Z"
        },
        "id": "ea9Qw2XX03Tv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch.cuda.empty_cache()\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "mUM6KgAM-oT1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.264732Z",
          "iopub.execute_input": "2025-05-17T22:49:14.264988Z",
          "iopub.status.idle": "2025-05-17T22:49:14.282471Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.264966Z",
          "shell.execute_reply": "2025-05-17T22:49:14.281673Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"71aebd5eed3e9b3e37a5a3c4658f5433375d97dc\")\n"
      ],
      "metadata": {
        "id": "KZ1QyWsV-oUN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.283697Z",
          "iopub.execute_input": "2025-05-17T22:49:14.283954Z",
          "iopub.status.idle": "2025-05-17T22:49:14.644451Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.283931Z",
          "shell.execute_reply": "2025-05-17T22:49:14.643785Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "HwQHFp9POlu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "'''Location of your CSV file (Extracted file)\n",
        "Location of your CSV file if on kaggle than zip file is fine'''\n",
        "\n",
        "trainFilepath=\"/kaggle/input/myinput/aksharantar_sampled/hin/hin_train.csv\"\n",
        "valFilePath=\"/kaggle/input/myinput/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "testFilePath=\"/kaggle/input/myinput/aksharantar_sampled/hin/hin_test.csv\""
      ],
      "metadata": {
        "id": "R-OebR_K-oUE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.645158Z",
          "iopub.execute_input": "2025-05-17T22:49:14.645816Z",
          "iopub.status.idle": "2025-05-17T22:49:14.649678Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.645792Z",
          "shell.execute_reply": "2025-05-17T22:49:14.649118Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_0(filepath):\n",
        "    def process_row(text_row):\n",
        "        return list(text_row)\n",
        "\n",
        "    def collect_characters(reader_obj):\n",
        "        all_chars = []\n",
        "        for entry in reader_obj:\n",
        "            all_chars += process_row(entry[0])\n",
        "        return all_chars\n",
        "\n",
        "    with open(filepath, mode='r') as file_handle:\n",
        "        csv_reader = csv.reader(file_handle)\n",
        "        return collect_characters(csv_reader)\n"
      ],
      "metadata": {
        "id": "lL5546mp03Tw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.651181Z",
          "iopub.execute_input": "2025-05-17T22:49:14.651435Z",
          "iopub.status.idle": "2025-05-17T22:49:14.664179Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.651420Z",
          "shell.execute_reply": "2025-05-17T22:49:14.663561Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# trainFilepath = '/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv'\n",
        "\n",
        "\n",
        "chars = read_file_0(trainFilepath)\n",
        "setChar=set(chars)\n",
        "setChar.add('|')\n",
        "setOfchar = list(setChar)\n",
        "\n",
        "# Create the association between characters and their corresponding integer indices\n",
        "char_to_idx_latin= {char: i+1 for i, char in enumerate(setOfchar)}"
      ],
      "metadata": {
        "id": "MuY2sxX_03Tx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.664952Z",
          "iopub.execute_input": "2025-05-17T22:49:14.665215Z",
          "iopub.status.idle": "2025-05-17T22:49:14.739111Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.665193Z",
          "shell.execute_reply": "2025-05-17T22:49:14.738637Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_1(trainFilepath):\n",
        "    def extract_chars(text):\n",
        "        return list(text)\n",
        "\n",
        "    def accumulate(reader_obj):\n",
        "        buffer = []\n",
        "        for entry in reader_obj:\n",
        "            buffer += extract_chars(entry[1])\n",
        "        return buffer\n",
        "\n",
        "    with open(trainFilepath, 'r') as file_stream:\n",
        "        csv_rows = csv.reader(file_stream)\n",
        "        return accumulate(csv_rows)"
      ],
      "metadata": {
        "id": "_Fu5-CC603Tx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.739701Z",
          "iopub.execute_input": "2025-05-17T22:49:14.740009Z",
          "iopub.status.idle": "2025-05-17T22:49:14.744580Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.739991Z",
          "shell.execute_reply": "2025-05-17T22:49:14.743922Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "maxLenDev=0\n",
        "\n",
        "chars = read_file_1(trainFilepath)\n",
        "setChar=set(chars)\n",
        "setChar.add('|')\n",
        "setOfchar = list(setChar)\n",
        "\n",
        "charToIndLang ={char: i+1 for i, char in enumerate(setOfchar)}"
      ],
      "metadata": {
        "id": "BsSPFp2f-oUF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.745284Z",
          "iopub.execute_input": "2025-05-17T22:49:14.745557Z",
          "iopub.status.idle": "2025-05-17T22:49:14.822937Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.745539Z",
          "shell.execute_reply": "2025-05-17T22:49:14.822271Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open(trainFilepath, 'r') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    max_length = 0\n",
        "\n",
        "    def update_max(current_max, candidate):\n",
        "        return candidate if candidate > current_max else current_max\n",
        "\n",
        "    for record in reader:\n",
        "        length = len(record[0])\n",
        "        max_length = update_max(max_length, length)\n",
        "\n",
        "    maxLenEng = max_length"
      ],
      "metadata": {
        "id": "YnH-KUwf-oUH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.823870Z",
          "iopub.execute_input": "2025-05-17T22:49:14.824137Z",
          "iopub.status.idle": "2025-05-17T22:49:14.871037Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.824105Z",
          "shell.execute_reply": "2025-05-17T22:49:14.870567Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open(trainFilepath, 'r') as csv_stream:\n",
        "    reader_obj = csv.reader(csv_stream)\n",
        "    longest = 0\n",
        "\n",
        "    def get_larger(a, b):\n",
        "        return b if b > a else a\n",
        "\n",
        "    for line in reader_obj:\n",
        "        current_length = len(line[1])\n",
        "        longest = get_larger(longest, current_length)\n",
        "\n",
        "    maxLenDev = longest"
      ],
      "metadata": {
        "id": "_CT0LTQL-oUI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T22:49:14.871609Z",
          "iopub.execute_input": "2025-05-17T22:49:14.871784Z",
          "iopub.status.idle": "2025-05-17T22:49:14.915759Z",
          "shell.execute_reply.started": "2025-05-17T22:49:14.871770Z",
          "shell.execute_reply": "2025-05-17T22:49:14.915089Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "characters in words -> indices"
      ],
      "metadata": {
        "id": "tsO0M3gVFjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_characters_to_indices(word, dictionary):\n",
        "    def safe_lookup(char):\n",
        "        return dictionary[char] if char in dictionary else -1\n",
        "\n",
        "    mapped = [safe_lookup(ch) for ch in word]\n",
        "    filtered = list(filter(lambda idx: idx >= 0, mapped))\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "sheJBSGMEUAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_sequence_length(indices, maximumLength):\n",
        "    current_length = len(indices)\n",
        "\n",
        "    def trim(seq, length):\n",
        "        return seq[:length]\n",
        "\n",
        "    def pad(seq, total_length):\n",
        "        padding_needed = total_length - len(seq)\n",
        "        return seq + [0] * padding_needed\n",
        "\n",
        "    if current_length > maximumLength:\n",
        "        return trim(indices, maximumLength)\n",
        "    elif current_length < maximumLength:\n",
        "        return pad(indices, maximumLength)\n",
        "    return indices\n"
      ],
      "metadata": {
        "id": "WX8Z8YnYEihz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_indices_to_tensor(indices, dictionary):\n",
        "    token = dictionary.get('|', 0)\n",
        "\n",
        "    def add_delimiters(seq, token_id):\n",
        "        return [token_id] + seq + [token_id]\n",
        "\n",
        "    sequence = add_delimiters(indices, token)\n",
        "    tensor_obj = torch.tensor(sequence, device=device)\n",
        "    return tensor_obj"
      ],
      "metadata": {
        "id": "XtOqUF5rEjVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_word_to_indices(word, maximumLength, dict):\n",
        "    char_ids = convert_characters_to_indices(word, dict)\n",
        "\n",
        "    def standardize_length(seq, target_len):\n",
        "        return adjust_sequence_length(seq, target_len)\n",
        "\n",
        "    resized = standardize_length(char_ids, maximumLength)\n",
        "    tensor_out = convert_indices_to_tensor(resized, dict)\n",
        "    return tensor_out\n"
      ],
      "metadata": {
        "id": "-KtrvJ-eEnm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_tensor_to_generated_sequences(sequence):\n",
        "    tokens = sequence.split()\n",
        "\n",
        "    def concatenate(tokens):\n",
        "        return ''.join(tokens)\n",
        "\n",
        "    def compute_total_length(tokens):\n",
        "        return sum(len(token) for token in tokens)\n",
        "\n",
        "    combined = concatenate(tokens)\n",
        "    total_len = compute_total_length(tokens)\n",
        "\n",
        "    return combined, total_len"
      ],
      "metadata": {
        "id": "G_7gFP9rEtAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_tensor(final_tensor, partition_size=1):\n",
        "    def safe_partition_size(size):\n",
        "        return max(1, size)\n",
        "\n",
        "    chunk_size = safe_partition_size(partition_size)\n",
        "    segments = [final_tensor[idx:idx + chunk_size] for idx in range(0, len(final_tensor), chunk_size)]\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "D_RlTwkJE-AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_assigned_generated_seq(path):\n",
        "    combined_seq, total_chars = assign_tensor_to_generated_sequences(path)\n",
        "\n",
        "    def determine_chunk_size(length, divisor=4):\n",
        "        return max(1, length // divisor)\n",
        "\n",
        "    segment_length = determine_chunk_size(total_chars)\n",
        "    partitioned = assemble_tensor(combined_seq, segment_length)\n",
        "\n",
        "    return partitioned\n"
      ],
      "metadata": {
        "id": "xZiXxSCzFK4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_indices(row):\n",
        "    src_text = row[0]\n",
        "    tgt_text = row[1]\n",
        "\n",
        "    def build_indexed_tensor(text, max_len, char_map):\n",
        "        return convert_word_to_indices(text, max_len, char_map)\n",
        "\n",
        "    source_tensor = build_indexed_tensor(src_text, maxLenEng, char_to_idx_latin)\n",
        "    target_tensor = build_indexed_tensor(tgt_text, maxLenDev, charToIndLang)\n",
        "\n",
        "    return source_tensor, target_tensor\n"
      ],
      "metadata": {
        "id": "IMihcYLNFLf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_v = []\n",
        "\n",
        "with open(valFilePath, 'r') as val_file:\n",
        "    csv_reader = csv.reader(val_file)\n",
        "\n",
        "    def process_and_append(pairs, record):\n",
        "        eng_tensor, hin_tensor = generate_indices(record)\n",
        "        pairs.append([eng_tensor, hin_tensor])\n",
        "\n",
        "    for entry in csv_reader:\n",
        "        process_and_append(pairs_v, entry)\n"
      ],
      "metadata": {
        "id": "CnuWgWf1FNY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_t = []\n",
        "\n",
        "with open(testFilePath, 'r') as test_file:\n",
        "    test_reader = csv.reader(test_file)\n",
        "\n",
        "    def append_indexed_pair(container, data_row):\n",
        "        src_tensor, tgt_tensor = generate_indices(data_row)\n",
        "        container.append([src_tensor, tgt_tensor])\n",
        "\n",
        "    for record in test_reader:\n",
        "        append_indexed_pair(pairs_t, record)\n"
      ],
      "metadata": {
        "id": "m-KmOYo8FZFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "\n",
        "with open(trainFilepath, 'r') as train_file:\n",
        "    train_reader = csv.reader(train_file)\n",
        "\n",
        "    def process_row_and_store(storage, row_data):\n",
        "        input_tensor, target_tensor = generate_indices(row_data)\n",
        "        storage.append([input_tensor, target_tensor])\n",
        "\n",
        "    for entry in train_reader:\n",
        "        process_row_and_store(pairs, entry)\n"
      ],
      "metadata": {
        "id": "lTOnM5UUFbr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Dataloaders"
      ],
      "metadata": {
        "id": "swFq-NNrFjWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_shuffle = True\n",
        "eval_shuffle = False\n",
        "\n",
        "def create_loader(dataset, size, shuffle_flag):\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=size, shuffle=shuffle_flag)\n",
        "\n",
        "dataloaderTrain = create_loader(pairs, batch_size, train_shuffle)\n",
        "dataloaderVal = create_loader(pairs_v, batch_size, eval_shuffle)\n",
        "dataloaderTest = create_loader(pairs_t, batch_size, eval_shuffle)\n"
      ],
      "metadata": {
        "id": "kPxbf6sKFeIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLASS FOR ENCODER DECODER"
      ],
      "metadata": {
        "id": "siTPUPE7FrT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Encoder class (no attention mechanism)\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, embed_size, cell_type, dropout_rate, num_layers, is_bidirectional):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "\n",
        "        self.embed_layer = nn.Embedding(input_dim, embed_size)\n",
        "\n",
        "        rnn_args = {\n",
        "            \"input_size\": embed_size,\n",
        "            \"hidden_size\": hidden_dim,\n",
        "            \"dropout\": dropout_rate,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"bidirectional\": is_bidirectional\n",
        "        }\n",
        "\n",
        "        if cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(**rnn_args)\n",
        "        elif cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(**rnn_args)\n",
        "        elif cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(**rnn_args)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded_seq = self.embed_layer(input_seq)\n",
        "        output, hidden_state = self.rnn(embedded_seq)\n",
        "        return hidden_state\n",
        "\n",
        "\n",
        "# Decoder class (no attention mechanism)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, embed_size, cell_type, dropout_rate, num_layers, is_bidirectional):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.cell_type = cell_type\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "\n",
        "        direction_factor = 2 if is_bidirectional else 1\n",
        "\n",
        "        self.embed_layer = nn.Embedding(output_dim, embed_size)\n",
        "\n",
        "        rnn_args = {\n",
        "            \"input_size\": embed_size,\n",
        "            \"hidden_size\": hidden_dim,\n",
        "            \"dropout\": dropout_rate,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"bidirectional\": is_bidirectional\n",
        "        }\n",
        "\n",
        "        if cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(**rnn_args)\n",
        "        elif cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(**rnn_args)\n",
        "        elif cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(**rnn_args)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dim * direction_factor, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden_state):\n",
        "        embedded = self.embed_layer(input_token.unsqueeze(0))  # shape: (1, batch, embed_size)\n",
        "        rnn_output, updated_hidden = self.rnn(embedded, hidden_state)\n",
        "        logits = self.output_layer(rnn_output.squeeze(0))  # remove time dimension\n",
        "        return logits, updated_hidden\n"
      ],
      "metadata": {
        "id": "SIKz6zhGFovz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SEQ2SEQ class"
      ],
      "metadata": {
        "id": "g4NYBZgoWDWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(pl.LightningModule):\n",
        "    def __init__(self, input_dim, output_dim, hidden_size, embed_size, rnn_type, dropout, enc_layers, dec_layers, is_bidirectional, lr,char_to_idx_latin, charToIndLang):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "        self.rnn_type = rnn_type\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "        self.dir_mult = 2 if is_bidirectional else 1\n",
        "        self.char2lat = char_to_idx_latin   # source vocab\n",
        "        self.char2hin = charToIndLang       # target vocab\n",
        "\n",
        "        # Initialize encoder and decoder\n",
        "        self.encoder = Encoder(input_dim, hidden_size, embed_size, rnn_type, dropout, enc_layers, is_bidirectional)\n",
        "        self.decoder = Decoder(output_dim, hidden_size, embed_size, rnn_type, dropout, dec_layers, is_bidirectional)\n",
        "\n",
        "        # Logging containers for metrics\n",
        "        self.train_loss_log, self.train_acc_log = [], []\n",
        "        self.val_loss_log, self.val_acc_log = [], []\n",
        "        self.test_loss_log, self.test_acc_log = [], []\n",
        "\n",
        "    def forward(self, src_seq, tgt_seq, teacher_prob=0.5):\n",
        "        # Sequence-to-sequence forward pass with teacher forcing\n",
        "        batch_size, seq_len = tgt_seq.shape\n",
        "        vocab_size = self.decoder.output_layer.out_features\n",
        "        prediction_store = torch.zeros(seq_len, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        src_seq = src_seq.transpose(0, 1)\n",
        "        hidden_state = self._fix_hidden_dims(self.encoder(src_seq))\n",
        "\n",
        "        current_token = tgt_seq[:, 0]\n",
        "        for step in range(1, seq_len):\n",
        "            pred_output, hidden_state = self.decoder(current_token, hidden_state)\n",
        "            prediction_store[step] = pred_output\n",
        "            current_token = pred_output.argmax(1) if teacher_prob < torch.rand(1).item() else tgt_seq[:, step]\n",
        "\n",
        "        return prediction_store\n",
        "\n",
        "    def _fix_hidden_dims(self, hidden):\n",
        "        # Adjust hidden layers if encoder and decoder layer counts mismatch\n",
        "        lstm_mode = self.rnn_type == \"LSTM\"\n",
        "        enc_more = self.enc_layers > self.dec_layers\n",
        "        dec_more = self.enc_layers < self.dec_layers\n",
        "\n",
        "        if enc_more:\n",
        "            cut = (self.enc_layers - self.dec_layers) * self.dir_mult\n",
        "            if lstm_mode:\n",
        "                h, c = hidden\n",
        "                return (h[cut:], c[cut:])\n",
        "            return hidden[cut:]\n",
        "\n",
        "        if dec_more:\n",
        "            add = self.dec_layers - self.enc_layers\n",
        "            if lstm_mode:\n",
        "                h, c = hidden\n",
        "                h_last, c_last = h[-self.dir_mult:], c[-self.dir_mult:]\n",
        "                h = torch.cat([h] + [h_last] * add, dim=0)\n",
        "                c = torch.cat([c] + [c_last] * add, dim=0)\n",
        "                return (h, c)\n",
        "            else:\n",
        "                h_last = hidden[-self.dir_mult:]\n",
        "                return torch.cat([hidden] + [h_last] * add, dim=0)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "    def forward_pass(self, src, tgt):\n",
        "        # Wraps model forward for clarity\n",
        "        return self(src, tgt)\n",
        "\n",
        "    def compute_expected_values(self, preds, target):\n",
        "        # Create one-hot encoded expected output tensor\n",
        "        result = torch.zeros_like(preds)\n",
        "        result[torch.arange(preds.shape[0]), torch.arange(preds.shape[1]).unsqueeze(1), target.cpu()] = 1\n",
        "        return result\n",
        "\n",
        "    def calculate_loss(self, preds, expected, target):\n",
        "        # Cross-entropy loss computation (skip BOS)\n",
        "        d = preds.shape[-1]\n",
        "        return self.loss_fn(preds[1:].reshape(-1, d), expected[1:].reshape(-1, d))\n",
        "\n",
        "    def calculate_accuracy(self, preds, target):\n",
        "        # Accuracy at word level (after permute)\n",
        "        preds = preds.permute(1, 0, 2)\n",
        "        return self._wordwise_accuracy(preds, target)\n",
        "\n",
        "    def update_metrics(self, loss_val, acc_val):\n",
        "        # Append training metrics to logs\n",
        "        self.train_loss_log.append(loss_val.detach())\n",
        "        self.train_acc_log.append(torch.tensor(acc_val))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Called during each training batch\n",
        "        src, tgt = batch\n",
        "        logits = self.forward_pass(src, tgt)\n",
        "        expected = self.compute_expected_values(logits, tgt)\n",
        "        loss = self.calculate_loss(logits, expected, tgt)\n",
        "        acc = self.calculate_accuracy(logits, tgt)\n",
        "        self.update_metrics(loss, acc)\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def forward_pass_validation(self, src, tgt):\n",
        "        # Run forward without teacher forcing\n",
        "        return self(src, tgt, teacher_prob=0.0)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Called for each validation batch\n",
        "        src, tgt = batch\n",
        "        logits = self.forward_pass_validation(src, tgt)\n",
        "        expected = self.compute_expected_values(logits, tgt)\n",
        "        loss = self.calculate_loss(logits, expected, tgt)\n",
        "        acc = self.calculate_accuracy(logits, tgt)\n",
        "        self.val_loss_log.append(loss.detach())\n",
        "        self.val_acc_log.append(torch.tensor(acc))\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        One full forward pass on the test set (no teacher forcing).\n",
        "        Computes and logs loss and word-level accuracy, and saves predictions.\n",
        "        \"\"\"\n",
        "        src_seq, tgt_seq = batch\n",
        "        logits = self(src_seq, tgt_seq, teacher_prob=0.0)\n",
        "        gold_one_hot = self.compute_expected_values(logits, tgt_seq)\n",
        "        loss_val = self.calculate_loss(logits, gold_one_hot, tgt_seq)\n",
        "        acc_val = self._wordwise_accuracy(logits.permute(1, 0, 2), tgt_seq)\n",
        "\n",
        "        # Save predictions to CSV\n",
        "        src_tokens, tgt_tokens, pred_tokens = self.grid(src_seq, logits.permute(1, 0, 2), tgt_seq)\n",
        "\n",
        "        def tokens_to_string(token_list, vocab):\n",
        "            return \"\".join([k for tok in token_list for k, v in vocab.items() if v == tok])\n",
        "\n",
        "        inputs = [tokens_to_string(seq.tolist(), self.char2lat) for seq in src_tokens]\n",
        "        targets = [tokens_to_string(seq.tolist(), self.char2hin) for seq in tgt_tokens]\n",
        "        predictions = [tokens_to_string(seq.tolist(), self.char2hin) for seq in pred_tokens]\n",
        "\n",
        "        save_outputs_to_csv(inputs, targets, predictions)\n",
        "\n",
        "        #uncomment below code to log predictions on test data\n",
        "        # log_predictions_to_wandb(inputs, targets, predictions)\n",
        "\n",
        "        self.test_loss_log.append(loss_val.detach())\n",
        "        self.test_acc_log.append(torch.tensor(acc_val))\n",
        "        return {\"loss\": loss_val}\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        # Print and log test epoch results\n",
        "        loss_avg = torch.stack(self.test_loss_log).mean()\n",
        "        acc_avg = torch.stack(self.test_acc_log).mean()\n",
        "        self.test_loss_log.clear()\n",
        "        self.test_acc_log.clear()\n",
        "        print({\"test_loss\": loss_avg.item(), \"testAccuracy\": acc_avg.item()})\n",
        "        wandb.log({\"test_loss_last\": loss_avg, \"testAccuracy_last\": acc_avg})\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # Print and log training & validation metrics\n",
        "        tr_loss = torch.stack(self.train_loss_log).mean()\n",
        "        val_loss = torch.stack(self.val_loss_log).mean()\n",
        "        tr_acc = torch.stack(self.train_acc_log).mean()\n",
        "        val_acc = torch.stack(self.val_acc_log).mean()\n",
        "\n",
        "        self.train_loss_log.clear()\n",
        "        self.val_loss_log.clear()\n",
        "        self.train_acc_log.clear()\n",
        "        self.val_acc_log.clear()\n",
        "\n",
        "        print({\n",
        "            \"Train Loss\": round(tr_loss.item(), 3),\n",
        "            \"Train Accuracy\": round(tr_acc.item(), 3),\n",
        "            \"Validation Loss\": round(val_loss.item(), 3),\n",
        "            \"Validation Accuracy\": round(val_acc.item(), 3)\n",
        "        })\n",
        "        wandb.log({\n",
        "            \"Train Loss\": tr_loss,\n",
        "            \"Train Accuracy\": tr_acc,\n",
        "            \"Validation Loss\": val_loss,\n",
        "            \"Validation Accuracy\": val_acc\n",
        "        })\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Adam optimizer setup\n",
        "        return optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def loss_fn(self, out, target):\n",
        "        # Cross-entropy loss wrapper\n",
        "        return nn.CrossEntropyLoss()(out, target).mean()\n",
        "\n",
        "    def _wordwise_accuracy(self, logits, gold):\n",
        "        # Computes word-level match accuracy\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        match_count = sum(torch.equal(gold[i, 1:-1], preds[i, 1:-1]) for i in range(gold.size(0)))\n",
        "        return (match_count / gold.size(0)) * 100\n",
        "\n",
        "    def grid(self, inputs, outputs, targets):\n",
        "        # Converts batch output into src-tgt-predicted token lists\n",
        "        guesses = outputs.argmax(dim=-1)\n",
        "        src_tokens, tgt_tokens, pred_tokens = [], [], []\n",
        "        for idx in range(targets.size(0)):\n",
        "            src_tokens.append(inputs[idx, 1:-1])\n",
        "            tgt_tokens.append(targets[idx, 1:-1])\n",
        "            pred_tokens.append(guesses[idx, 1:-1])\n",
        "        return src_tokens, tgt_tokens, pred_tokens\n"
      ],
      "metadata": {
        "id": "7Xx2EoLBWC3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_outputs_to_csv(inputs, targets, predictions):\n",
        "    output_file = 'Output.csv'\n",
        "    already_present = os.path.isfile(output_file)\n",
        "\n",
        "    records = {\n",
        "        'input': inputs,\n",
        "        'target': targets,\n",
        "        'predicted': predictions\n",
        "    }\n",
        "\n",
        "    dataframe = pd.DataFrame(records)\n",
        "    dataframe.to_csv(output_file, mode='a', index=False, header=not already_present)\n",
        "\n",
        "\n",
        "def log_predictions_to_wandb(inputs, targets, preds):\n",
        "    prediction_table = wandb.Table(columns=[\"Input\", \"Target\", \"Prediction Result\"])\n",
        "\n",
        "    for src, gold, guess in zip(inputs, targets, preds):\n",
        "        result = \"✅\" if gold == guess else \"❌\"\n",
        "        labeled_pred = f\"{guess} {result}\"\n",
        "        prediction_table.add_data(src, gold, labeled_pred)\n",
        "\n",
        "    wandb.log({\"Predictions Overview\": prediction_table})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function will return key for given value\n",
        "def index_to_char_latin(index):\n",
        "    return next((char for char, idx in char_to_idx_latin.items() if idx == index), \"\")\n",
        "\n",
        "def index_to_char_lang(index):\n",
        "    return next((char for char, idx in charToIndLang.items() if idx == index), \"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pzf2bkrsWYCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SWEEP CONFIG"
      ],
      "metadata": {
        "id": "i7KWxX5hWfpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config_NOAttn = {\n",
        "    # Bayesian Search for hyperparameters\n",
        "    \"name\" : \"Bayesian Sweep\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Validation Accuracy\"},\n",
        "    \"parameters\": {'drop_out': {\"values\": [0.3,0.5]},\n",
        "                   'embeddingSize': {\"values\": [64,128,256]},\n",
        "                   'hidden_layer_size': {\"values\": [128,256,512]},\n",
        "                   'layersEncoder': {\"values\": [2, 3]},\n",
        "                   'layersDecoder': {\"values\": [2, 3]},\n",
        "                   \"cellType\": {\"values\": [ \"RNN\", \"GRU\", \"LSTM\"]},\n",
        "                   \"learningRate\": {\"values\": [1e-3, 0.005]},\n",
        "                   \"bidirectional\":{\"values\":[True, False]},\n",
        "                   \"epochs\": {\"values\": [10, 15]}\n",
        "                }\n",
        "}"
      ],
      "metadata": {
        "id": "-5dSSw0JWfTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_no_attn():\n",
        "    run = wandb.init(project=\"DA6401_ASS3_VANILLA\", name=\"test_VANILLA_grid\")\n",
        "    config = run.config\n",
        "\n",
        "    # Set a descriptive name for this specific run\n",
        "    wandb.run.name = (\n",
        "        f\"_cell_{config.cellType}_dr_{config.drop_out}\"\n",
        "        f\"_em_{config.embeddingSize}_hl_{config.hidden_layer_size}\"\n",
        "        f\"_en_{config.layersEncoder}_de_{config.layersDecoder}\"\n",
        "        f\"_lr_{config.learningRate}_bi_{config.bidirectional}\"\n",
        "        f\"_ep_{config.epochs}\"\n",
        "    )\n",
        "\n",
        "    # Extract hyperparameters\n",
        "    model_params = {\n",
        "        \"input_dim\": len(char_to_idx_latin) + 2,\n",
        "        \"output_dim\": len(charToIndLang) + 2,\n",
        "        \"hidden_dim\": config.hidden_layer_size,\n",
        "        \"embedding_dim\": config.embeddingSize,\n",
        "        \"cell_type\": config.cellType,\n",
        "        \"dropout\": config.drop_out,\n",
        "        \"enc_layers\": config.layersEncoder,\n",
        "        \"dec_layers\": config.layersDecoder,\n",
        "        \"bidirectional\": config.bidirectional,\n",
        "        \"lr\": config.learningRate\n",
        "    }\n",
        "\n",
        "    # Model initialization (no attention)\n",
        "    model = Seq2Seq(\n",
        "        model_params[\"input_dim\"],\n",
        "        model_params[\"output_dim\"],\n",
        "        model_params[\"hidden_dim\"],\n",
        "        model_params[\"embedding_dim\"],\n",
        "        model_params[\"cell_type\"],\n",
        "        model_params[\"dropout\"],\n",
        "        model_params[\"enc_layers\"],\n",
        "        model_params[\"dec_layers\"],\n",
        "        model_params[\"bidirectional\"],\n",
        "        model_params[\"lr\"]\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Train the model using PyTorch Lightning\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=config.epochs,\n",
        "        accelerator=\"gpu\",\n",
        "        devices=1\n",
        "    )\n",
        "    trainer.fit(model, train_dataloaders=dataloaderTrain, val_dataloaders=dataloaderVal)\n"
      ],
      "metadata": {
        "id": "Vl1b-nAlWicX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Running Sweeps for no attention\n",
        "sweep_id = wandb.sweep(sweep_config_NOAttn, project='DA6401_ASS3_VANILLA')\n",
        "wandb.agent(sweep_id, train_no_attn, count = 40)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "BE4dma8DWnjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BEST PARAMS\n"
      ],
      "metadata": {
        "id": "HO9-8ieAWuuz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qsp78CKBWw72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}